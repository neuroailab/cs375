{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS375 - Tutorial 2\n",
    "\n",
    "### Welcome to tutorial 2! This tutorial will introduce you to unsupervised learning methods, specifically how to train a sparse autoencoder and variational autoencoder on MNIST, and how to evaluate the trained model on neural data. As before everything will be implemented using TFUtils. We will start with a sparse autoencoder and then move on to variational autoencoders.\n",
    "\n",
    "## 1.) Training and evaluating a sparse autoencoder on MNIST\n",
    "\n",
    "### 1.1.) Define a simple sparse autoencoder consisting of one fully connected layer in the encoder and one fully connected layer in the decoder. The input dimension is 784. Use xavier initialization and l1-regularization on the weights, initialize all biases to 0 and use a tanh activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "from tfutils import base, data, optimizer, utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pymongo as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "\n",
    "# connect to database\n",
    "dbname = 'mnist'\n",
    "collname = 'autoencoder'\n",
    "port = 24444\n",
    "conn = pm.MongoClient(port = port)\n",
    "coll = conn[dbname][collname + '.files']\n",
    "\n",
    "def sparse_autoencoder(inputs, train=True, beta = 5e-4, n_hidden = 100, **kwargs): \n",
    "    '''\n",
    "    Implements a simple autoencoder consisting of two fully connected layers\n",
    "    '''\n",
    "    # flatten the input images\n",
    "    inp = tf.reshape(inputs['images'], [inputs['images'].get_shape().as_list()[0], -1])\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return output, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.) Define the l2 loss function for the sparse autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_autoencoder_loss(inputs, outputs, **kwargs):\n",
    "    '''\n",
    "    Defines the loss = l2(inputs - outputs) + l1(weights)\n",
    "    '''\n",
    "    # flatten the input images\n",
    "    inputs = tf.reshape(inputs, [inputs.get_shape().as_list()[0], -1])\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def sparse_autoencoder_validation(inputs, outputs, **kwargs):\n",
    "    '''\n",
    "    Wrapper for using the loss function as a validation target\n",
    "    '''\n",
    "    return {'l2_loss': sparse_autoencoder_loss(inputs['images'], outputs),\n",
    "            'pred': outputs,\n",
    "            'gt': inputs['images']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define and run our sparse autoencoder experiment on MNIST in TFUtils. We will use the Adam optimizer and an exponentially decaying learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def online_agg_mean(agg_res, res, step):\n",
    "    \"\"\"\n",
    "    Appends the mean value for each key\n",
    "    \"\"\"\n",
    "    if agg_res is None:\n",
    "        agg_res = {k: [] for k in res}\n",
    "    for k, v in res.items():\n",
    "        if k in ['pred', 'gt']:\n",
    "            value = v\n",
    "        else:\n",
    "            value = np.mean(v)\n",
    "        agg_res[k].append(value)\n",
    "    return agg_res\n",
    "\n",
    "def agg_mean(results):\n",
    "    for k in results:\n",
    "        if k in ['pred', 'gt']:\n",
    "            results[k] = results[k][0]\n",
    "        elif k is 'l2_loss':\n",
    "            results[k] = np.mean(results[k])\n",
    "        else:\n",
    "            raise KeyError('Unknown target')\n",
    "    return results\n",
    "\n",
    "# number of hidden neurons\n",
    "n_hidden = 100\n",
    "# scaling of l1 regularization\n",
    "beta = 1e-2 # 1e-4 = no regularization\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'do_restore': False,\n",
    "}\n",
    "\n",
    "params['save_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'autoencoder',\n",
    "    'exp_id': 'exp1',\n",
    "    'save_metrics_freq': 200,\n",
    "    'save_valid_freq': 200,\n",
    "    'save_filters_freq': 1000,\n",
    "    'cache_filters_freq': 1000,\n",
    "}\n",
    "\n",
    "params['train_params'] = {\n",
    "    'validate_first': False,\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 256,\n",
    "                    'group': 'train',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'random',\n",
    "                     'batch_size': 256},\n",
    "    'num_steps': 4000,\n",
    "    'thres_loss': float(\"inf\"),\n",
    "}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 100,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 100},\n",
    "    'num_steps': 100,\n",
    "    'targets': {'func': sparse_autoencoder_validation},\n",
    "    'online_agg_func': online_agg_mean,\n",
    "    'agg_func': agg_mean,\n",
    "}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': sparse_autoencoder,\n",
    "    'beta': beta,\n",
    "    'n_hidden': n_hidden,\n",
    "} \n",
    "\n",
    "params['learning_rate_params'] = {\n",
    "    'learning_rate': 5e-3,\n",
    "    'decay_steps': 2000,\n",
    "    'decay_rate': 0.95,\n",
    "    'staircase': True,\n",
    "}\n",
    "\n",
    "params['optimizer_params'] = {\n",
    "    'func': optimizer.ClipOptimizer,\n",
    "    'optimizer_class': tf.train.AdamOptimizer,\n",
    "    'clip': False,\n",
    "}\n",
    "\n",
    "params['loss_params'] = {\n",
    "    'targets': ['images'],\n",
    "    'loss_per_case_func': sparse_autoencoder_loss,\n",
    "    'loss_per_case_func_params' : {'_outputs': 'outputs', '_targets_$all': 'inputs'},\n",
    "    'agg_func': tf.reduce_mean,\n",
    "}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "coll.remove({'exp_id' : 'exp1'}, {'justOne': True})\n",
    "base.train_from_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's have a look at our training and validation curves that were stored in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(exp_id):\n",
    "    \"\"\"\n",
    "    Gets all loss entries from the database and concatenates them into a vector\n",
    "    \"\"\"\n",
    "    q_train = {'exp_id' : exp_id, 'train_results' : {'$exists' : True}}\n",
    "    return np.array([_r['loss'] \n",
    "                     for r in coll.find(q_train, projection = ['train_results']) \n",
    "                     for _r in r['train_results']])\n",
    "\n",
    "def plot_train_loss(exp_id, start_step=None, end_step=None, N_smooth = 100, plt_title = None):\n",
    "    \"\"\"\n",
    "    Plots the training loss\n",
    "    \n",
    "    You will need to EDIT this part.\n",
    "    \"\"\"\n",
    "    # get the losses from the database\n",
    "    loss = get_losses(exp_id)\n",
    "\n",
    "    if start_step is None:\n",
    "        start_step = 0\n",
    "    if end_step is None:\n",
    "        end_step = len(loss)\n",
    "    if plt_title is None:\n",
    "        plt_title = exp_id\n",
    "    \n",
    "    # Only plot selected loss window\n",
    "    loss = loss[start_step:end_step]\n",
    "    \n",
    "    # plot loss\n",
    "    fig = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(loss)\n",
    "    plt.title(plt_title + ' training: loss')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "    # plot smoothed loss\n",
    "    smoothed_loss = signal.convolve(loss, np.ones((N_smooth,)))[N_smooth : -N_smooth] / float(N_smooth)\n",
    "    plt.figure(figsize = (15, 6))\n",
    "    plt.plot(smoothed_loss)\n",
    "    plt.title(plt_title + ' training: loss smoothed')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "plot_train_loss('exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data(exp_id):\n",
    "    \"\"\"\n",
    "    Gets the validation data from the database (except for gridfs data)\n",
    "    \"\"\"\n",
    "    q_val = {'exp_id' : exp_id, 'validation_results' : {'$exists' : True}, 'validates' : {'$exists' : False}}\n",
    "    val_steps = coll.find(q_val, projection = ['validation_results'])\n",
    "    return [val_steps[i]['validation_results']['valid0']['l2_loss'] \n",
    "            for i in range(val_steps.count())]\n",
    "\n",
    "def plot_validation_results(exp_id, plt_title = None):\n",
    "    \"\"\"\n",
    "    Plots the validation results i.e. the top1 and top5 accuracy\n",
    "    \n",
    "    You will need to EDIT this part.\n",
    "    \"\"\"\n",
    "    # get the data from the database\n",
    "    l2_loss = get_validation_data(exp_id)\n",
    "    \n",
    "    if plt_title is None:\n",
    "        plt_title = exp_id\n",
    "    \n",
    "    # plot top1 accuracy\n",
    "    fig = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(l2_loss)\n",
    "    plt.title(plt_title + ' validation: l2 loss')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "plot_validation_results('exp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally let's plot some example outputs of our autoencoder. The images to the left are the outputs. The images to the right are the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_images(exp_id):\n",
    "    \"\"\"\n",
    "    Gets the validation images from the database\n",
    "    \"\"\"\n",
    "    q_val = {'exp_id' : exp_id, 'validation_results' : {'$exists' : True}, 'validates' : {'$exists' : False}}\n",
    "    val_steps = coll.find(q_val, projection = ['validation_results'])\n",
    "    pred = np.array([val_steps[i]['validation_results']['valid0']['pred'] \n",
    "            for i in range(val_steps.count())])\n",
    "    gt = np.array([val_steps[i]['validation_results']['valid0']['gt'] \n",
    "            for i in range(val_steps.count())])\n",
    "    return {'gt': gt, 'pred': pred}\n",
    "\n",
    "def plot_validation_images(exp_id, n_images = 24):\n",
    "    '''\n",
    "    Plots n_images images in a grid. The ground truth image is on the left \n",
    "    and the prediction is on the right.\n",
    "    '''\n",
    "    imgs = get_validation_images(exp_id)\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    for i in range(n_images):\n",
    "        pred = np.reshape(imgs['pred'][0,i], [28, 28])\n",
    "        plt.subplot(n_images/4,n_images/3,1 + i*2)\n",
    "        plt.imshow(pred, cmap='gray')\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "        gt = np.reshape(imgs['gt'][0,i], [28, 28])\n",
    "        plt.subplot(n_images/4,n_images/3,2 + i*2)\n",
    "        plt.imshow(gt, cmap='gray')\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "plot_validation_images('exp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Training and evaluating a variational autoencoder on MNIST\n",
    "### 2.1.) Define a simple variational autoencoder as discussed in class consisting of  \n",
    "### - one fully connected layer (dimension n_hidden) in the encoder that results in the latent variable, \n",
    "### - two fully connected layers  (dimension n_latent) that take the latent variable and reparametrize the latent distribution with it's mean mu and log of the standard deviation, and\n",
    "### - two fully connected layers  (dimension n_hidden & output_dim) in the decoder that take the reparametrized latent variable and decode it into the output prediction. \n",
    "### The input dimension is 784. Use tanh activation functions in the intermediate layers and a sigmoid at the top layer. Use xavier initialization for the weights and constant initialization to 0 for the biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_autoencoder(inputs, train=True, n_hidden = 100, n_latent = 20, **kwargs): \n",
    "    '''\n",
    "    Implements a simple autoencoder consisting of two fully connected layers\n",
    "    '''\n",
    "    outputs = {}\n",
    "    # flatten the input images\n",
    "    inp = tf.reshape(inputs['images'], [inputs['images'].get_shape().as_list()[0], -1])\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "        \n",
    "    return outputs, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.) Define the loss for our variational autoencoder as discussed in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_autoencoder_loss(inputs, outputs, **kwargs):\n",
    "    '''\n",
    "    Defines the loss = l2(inputs - outputs) + l2(weights)\n",
    "    '''\n",
    "    # flatten the input images\n",
    "    inputs = tf.reshape(inputs, [inputs.get_shape().as_list()[0], -1])\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def variational_autoencoder_validation(inputs, outputs, **kwargs):\n",
    "    '''\n",
    "    Wrapper for using the loss function as a validation target\n",
    "    '''\n",
    "    return {'l2_loss': variational_autoencoder_loss(inputs['images'], outputs),\n",
    "            'pred': outputs['pred'],\n",
    "            'gt': inputs['images']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_agg_mean(agg_res, res, step):\n",
    "    \"\"\"\n",
    "    Appends the mean value for each key\n",
    "    \"\"\"\n",
    "    if agg_res is None:\n",
    "        agg_res = {k: [] for k in res}\n",
    "    for k, v in res.items():\n",
    "        if k in ['pred', 'gt']:\n",
    "            value = v\n",
    "        else:\n",
    "            value = np.mean(v)\n",
    "        agg_res[k].append(value)\n",
    "    return agg_res\n",
    "\n",
    "def agg_mean(results):\n",
    "    for k in results:\n",
    "        if k in ['pred', 'gt']:\n",
    "            results[k] = results[k][0]\n",
    "        elif k is 'l2_loss':\n",
    "            results[k] = np.mean(results[k])\n",
    "        else:\n",
    "            raise KeyError('Unknown target')\n",
    "    return results\n",
    "\n",
    "# number of hidden neurons\n",
    "n_hidden = 100\n",
    "# dimension of latent space\n",
    "n_latent = 20\n",
    "\n",
    "params = {}\n",
    "\n",
    "params['load_params'] = {\n",
    "    'do_restore': False,\n",
    "}\n",
    "\n",
    "params['save_params'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': 24444,\n",
    "    'dbname': 'mnist',\n",
    "    'collname': 'autoencoder',\n",
    "    'exp_id': 'exp2',\n",
    "    'save_metrics_freq': 200,\n",
    "    'save_valid_freq': 200,\n",
    "    'save_filters_freq': 1000,\n",
    "    'cache_filters_freq': 1000,\n",
    "}\n",
    "\n",
    "params['train_params'] = {\n",
    "    'validate_first': False,\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 256,\n",
    "                    'group': 'train',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'random',\n",
    "                     'batch_size': 256},\n",
    "    'num_steps': 10000,\n",
    "    'thres_loss': float(\"inf\"),\n",
    "}\n",
    "\n",
    "params['validation_params'] = {'valid0': {\n",
    "    'data_params': {'func': data.MNIST,\n",
    "                    'batch_size': 100,\n",
    "                    'group': 'test',\n",
    "                    'n_threads': 1},\n",
    "    'queue_params': {'queue_type': 'fifo',\n",
    "                     'batch_size': 100},\n",
    "    'num_steps': 100,\n",
    "    'targets': {'func': variational_autoencoder_validation},\n",
    "    'online_agg_func': online_agg_mean,\n",
    "    'agg_func': agg_mean,\n",
    "}}\n",
    "\n",
    "params['model_params'] = {\n",
    "    'func': variational_autoencoder,\n",
    "    'n_latent': n_latent,\n",
    "    'n_hidden': n_hidden,\n",
    "} \n",
    "\n",
    "params['learning_rate_params'] = {\n",
    "    'learning_rate': 5e-3,\n",
    "    'decay_steps': 10000,\n",
    "    'decay_rate': 0.95,\n",
    "    'staircase': True,\n",
    "}\n",
    "\n",
    "params['optimizer_params'] = {\n",
    "    'func': optimizer.ClipOptimizer,\n",
    "    'optimizer_class': tf.train.AdamOptimizer,\n",
    "    'clip': True,\n",
    "}\n",
    "\n",
    "params['loss_params'] = {\n",
    "    'targets': ['images'],\n",
    "    'loss_per_case_func': variational_autoencoder_loss,\n",
    "    'loss_per_case_func_params' : {'_outputs': 'outputs', '_targets_$all': 'inputs'},\n",
    "    'agg_func': tf.reduce_mean,\n",
    "}\n",
    "\n",
    "params['skip_check'] = True\n",
    "\n",
    "coll.remove({'exp_id' : 'exp2'}, {'justOne': True})\n",
    "base.train_from_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's have a look at our training and validation curves that were stored in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(exp_id):\n",
    "    \"\"\"\n",
    "    Gets all loss entries from the database and concatenates them into a vector\n",
    "    \"\"\"\n",
    "    q_train = {'exp_id' : exp_id, 'train_results' : {'$exists' : True}}\n",
    "    return np.array([_r['loss'] \n",
    "                     for r in coll.find(q_train, projection = ['train_results']) \n",
    "                     for _r in r['train_results']])\n",
    "\n",
    "def plot_train_loss(exp_id, start_step=None, end_step=None, N_smooth = 100, plt_title = None):\n",
    "    \"\"\"\n",
    "    Plots the training loss\n",
    "    \n",
    "    You will need to EDIT this part.\n",
    "    \"\"\"\n",
    "    # get the losses from the database\n",
    "    loss = get_losses(exp_id)\n",
    "\n",
    "    if start_step is None:\n",
    "        start_step = 0\n",
    "    if end_step is None:\n",
    "        end_step = len(loss)\n",
    "    if plt_title is None:\n",
    "        plt_title = exp_id\n",
    "    \n",
    "    # Only plot selected loss window\n",
    "    loss = loss[start_step:end_step]\n",
    "    \n",
    "    # plot loss\n",
    "    fig = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(loss)\n",
    "    plt.title(plt_title + ' training: loss')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "    # plot smoothed loss\n",
    "    smoothed_loss = signal.convolve(loss, np.ones((N_smooth,)))[N_smooth : -N_smooth] / float(N_smooth)\n",
    "    plt.figure(figsize = (15, 6))\n",
    "    plt.plot(smoothed_loss)\n",
    "    plt.title(plt_title + ' training: loss smoothed')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "plot_train_loss('exp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data(exp_id):\n",
    "    \"\"\"\n",
    "    Gets the validation data from the database (except for gridfs data)\n",
    "    \"\"\"\n",
    "    q_val = {'exp_id' : exp_id, 'validation_results' : {'$exists' : True}, 'validates' : {'$exists' : False}}\n",
    "    val_steps = coll.find(q_val, projection = ['validation_results'])\n",
    "    return [val_steps[i]['validation_results']['valid0']['l2_loss'] \n",
    "            for i in range(val_steps.count())]\n",
    "\n",
    "def plot_validation_results(exp_id, plt_title = None):\n",
    "    \"\"\"\n",
    "    Plots the validation results i.e. the top1 and top5 accuracy\n",
    "    \n",
    "    You will need to EDIT this part.\n",
    "    \"\"\"\n",
    "    # get the data from the database\n",
    "    l2_loss = get_validation_data(exp_id)\n",
    "    \n",
    "    if plt_title is None:\n",
    "        plt_title = exp_id\n",
    "    \n",
    "    # plot top1 accuracy\n",
    "    fig = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(l2_loss)\n",
    "    plt.title(plt_title + ' validation: l2 loss')\n",
    "    plt.grid()\n",
    "    axes = plt.gca()\n",
    "\n",
    "plot_validation_results('exp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally let's plot some example outputs of our autoencoder. The images to the left are the outputs. The images to the right are the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_images(exp_id):\n",
    "    \"\"\"\n",
    "    Gets the validation images from the database\n",
    "    \"\"\"\n",
    "    q_val = {'exp_id' : exp_id, 'validation_results' : {'$exists' : True}, 'validates' : {'$exists' : False}}\n",
    "    val_steps = coll.find(q_val, projection = ['validation_results'])\n",
    "    pred = np.array([val_steps[i]['validation_results']['valid0']['pred'] \n",
    "            for i in range(val_steps.count())])\n",
    "    gt = np.array([val_steps[i]['validation_results']['valid0']['gt'] \n",
    "            for i in range(val_steps.count())])\n",
    "    return {'gt': gt, 'pred': pred}\n",
    "\n",
    "def plot_validation_images(exp_id, n_images = 24):\n",
    "    '''\n",
    "    Plots n_images images in a grid. The ground truth image is on the left \n",
    "    and the prediction is on the right.\n",
    "    '''\n",
    "    imgs = get_validation_images(exp_id)\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    for i in range(n_images):\n",
    "        pred = np.reshape(imgs['pred'][0,i], [28, 28])\n",
    "        plt.subplot(n_images/4,n_images/3,1 + i*2)\n",
    "        plt.imshow(pred, cmap='gray')\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "        gt = np.reshape(imgs['gt'][0,i], [28, 28])\n",
    "        plt.subplot(n_images/4,n_images/3,2 + i*2)\n",
    "        plt.imshow(gt, cmap='gray')\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "plot_validation_images('exp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
